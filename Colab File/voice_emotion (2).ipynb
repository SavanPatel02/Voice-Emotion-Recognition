{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and mount a drive"
      ],
      "metadata": {
        "id": "MkSuR18W44TJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUSYWegILlTw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Trainer , TrainingArguments, Wav2Vec2BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saCUyLRMTrrg",
        "outputId": "30ba3703-414f-4762-bda5-74e70ca748c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/voice/archive (9)/TESS Toronto emotional speech set data\"\n"
      ],
      "metadata": {
        "id": "4tU1PZD0ZH8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "7RBy0vZMZJoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/voice/archive (9)/TESS Toronto emotional speech set data\""
      ],
      "metadata": {
        "id": "c3VKQjGYZNqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features"
      ],
      "metadata": {
        "id": "gQKkTI2F5Igo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract MFCC features\n",
        "def extract_features(file_path, max_len=40):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mfcc = np.mean(mfcc.T, axis=0)\n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "FehQzNRhwkwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "emotions = []\n",
        "features = []\n",
        "\n",
        "for folder in os.listdir(dataset_path):\n",
        "    folder_path = os.path.join(dataset_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                feature = extract_features(file_path)\n",
        "                features.append(feature)\n",
        "                emotions.append(folder.split('_')[1])  # Extract emotion from folder name"
      ],
      "metadata": {
        "id": "vyLBxVs-woBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Covnert DataFrame"
      ],
      "metadata": {
        "id": "E4RelOdz5Q3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "data = pd.DataFrame(features)\n",
        "data['emotion'] = emotions\n",
        "\n",
        "# Encode target labels\n",
        "le = LabelEncoder()\n",
        "data['emotion'] = le.fit_transform(data['emotion'])"
      ],
      "metadata": {
        "id": "wpONwSKAxP6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data"
      ],
      "metadata": {
        "id": "-f32anBo5UAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X = data.drop(columns=['emotion'])\n",
        "y = data['emotion']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "yQmkD3FPxUe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "Ty0W4YEjxaBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5QmFIz31-89",
        "outputId": "e0f64dc1-923b-42e8-8088-ab454a40fef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "}\n"
      ],
      "metadata": {
        "id": "8o0Osu67xcne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Train"
      ],
      "metadata": {
        "id": "nOkFz5Cc6LPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training & Evaluation\n",
        "import joblib\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name} Accuracy: {acc * 100:.2f}%\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "    # Save model\n",
        "    model_filename = f\"{name.replace(' ', '_')}.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"{name} model saved as {model_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1-ENJxdxhMe",
        "outputId": "789f0e79-5fbb-487f-f89b-2909fbd66937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Accuracy: 99.46%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fear       1.00      0.97      0.99        40\n",
            "    Pleasant       1.00      0.96      0.98        47\n",
            "         Sad       1.00      1.00      1.00        49\n",
            "       angry       0.99      1.00      0.99        70\n",
            "     disgust       1.00      1.00      1.00        89\n",
            "        fear       1.00      1.00      1.00        30\n",
            "       happy       0.97      1.00      0.99        77\n",
            "     neutral       1.00      1.00      1.00        82\n",
            "    pleasant       1.00      1.00      1.00        37\n",
            "         sad       1.00      1.00      1.00        39\n",
            "\n",
            "    accuracy                           0.99       560\n",
            "   macro avg       1.00      0.99      0.99       560\n",
            "weighted avg       0.99      0.99      0.99       560\n",
            "\n",
            "Random Forest model saved as Random_Forest.pkl\n",
            "\n",
            "Logistic Regression Accuracy: 99.46%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fear       1.00      1.00      1.00        40\n",
            "    Pleasant       1.00      0.98      0.99        47\n",
            "         Sad       1.00      1.00      1.00        49\n",
            "       angry       1.00      1.00      1.00        70\n",
            "     disgust       0.99      0.98      0.98        89\n",
            "        fear       1.00      1.00      1.00        30\n",
            "       happy       0.97      1.00      0.99        77\n",
            "     neutral       1.00      1.00      1.00        82\n",
            "    pleasant       1.00      1.00      1.00        37\n",
            "         sad       1.00      1.00      1.00        39\n",
            "\n",
            "    accuracy                           0.99       560\n",
            "   macro avg       1.00      1.00      1.00       560\n",
            "weighted avg       0.99      0.99      0.99       560\n",
            "\n",
            "Logistic Regression model saved as Logistic_Regression.pkl\n",
            "\n",
            "KNN Accuracy: 99.64%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fear       1.00      1.00      1.00        40\n",
            "    Pleasant       0.98      0.98      0.98        47\n",
            "         Sad       1.00      1.00      1.00        49\n",
            "       angry       1.00      1.00      1.00        70\n",
            "     disgust       1.00      1.00      1.00        89\n",
            "        fear       1.00      1.00      1.00        30\n",
            "       happy       0.99      0.99      0.99        77\n",
            "     neutral       1.00      1.00      1.00        82\n",
            "    pleasant       1.00      1.00      1.00        37\n",
            "         sad       1.00      1.00      1.00        39\n",
            "\n",
            "    accuracy                           1.00       560\n",
            "   macro avg       1.00      1.00      1.00       560\n",
            "weighted avg       1.00      1.00      1.00       560\n",
            "\n",
            "KNN model saved as KNN.pkl\n",
            "\n",
            "XGBoost Accuracy: 99.64%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fear       1.00      0.97      0.99        40\n",
            "    Pleasant       1.00      0.98      0.99        47\n",
            "         Sad       1.00      1.00      1.00        49\n",
            "       angry       0.99      1.00      0.99        70\n",
            "     disgust       1.00      1.00      1.00        89\n",
            "        fear       1.00      1.00      1.00        30\n",
            "       happy       0.99      1.00      0.99        77\n",
            "     neutral       1.00      1.00      1.00        82\n",
            "    pleasant       1.00      1.00      1.00        37\n",
            "         sad       1.00      1.00      1.00        39\n",
            "\n",
            "    accuracy                           1.00       560\n",
            "   macro avg       1.00      1.00      1.00       560\n",
            "weighted avg       1.00      1.00      1.00       560\n",
            "\n",
            "XGBoost model saved as XGBoost.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict emotion from new audio file\n",
        "def predict_emotion(audio_file, model):\n",
        "    feature = extract_features(audio_file).reshape(1, -1)\n",
        "    feature = scaler.transform(feature)\n",
        "    pred = model.predict_proba(feature)[0] * 100  # Get probability\n",
        "    emotion_confidence = dict(zip(le.classes_, pred))\n",
        "    return emotion_confidence"
      ],
      "metadata": {
        "id": "DpnFVs-IxrtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "Ign2adh06PTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_scores = predict_emotion(\"//content/drive/MyDrive/voice/archive (9)/TESS Toronto emotional speech set data/OAF_disgust/OAF_back_disgust.wav\", models['XGBoost'])\n",
        "print(emotion_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiE11Lxixw5K",
        "outputId": "1ca304c7-160b-4ecb-8bc5-ab1fb6e3151d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Fear': np.float32(0.0018480427), 'Pleasant': np.float32(0.0010227247), 'Sad': np.float32(0.0030001756), 'angry': np.float32(0.00082780677), 'disgust': np.float32(99.98108), 'fear': np.float32(0.0058437856), 'happy': np.float32(0.0012023264), 'neutral': np.float32(0.0019782935), 'pleasant': np.float32(0.0014203935), 'sad': np.float32(0.001788409)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEL4vHO_UvUE",
        "outputId": "57b1b94d-df29-41c8-a67a-981bb324b94e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.25.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.25.2-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.25.2 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Dashboard"
      ],
      "metadata": {
        "id": "boMTk24r6T43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install gradio librosa xgboost scikit-learn matplotlib\n",
        "\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Set dataset path from Google Drive\n",
        "# dataset_path = \"/content/drive/MyDrive/voice/archive (9)/TESS Toronto emotional speech set data\"\n",
        "\n",
        "# ----------- Feature Extraction ------------\n",
        "def extract_features(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=3, offset=0.5)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "    return mfccs_scaled\n",
        "\n",
        "# ----------- Load and Process Dataset ------------\n",
        "def load_data():\n",
        "    features = []\n",
        "    emotions = []\n",
        "\n",
        "    for dirpath, _, filenames in os.walk(dataset_path):\n",
        "        for file in filenames:\n",
        "            if file.endswith('.wav'):\n",
        "                emotion = file.split('_')[-1].replace('.wav', '')\n",
        "                file_path = os.path.join(dirpath, file)\n",
        "                feature = extract_features(file_path)\n",
        "                features.append(feature)\n",
        "                emotions.append(emotion)\n",
        "\n",
        "    X = np.array(features)\n",
        "    y = np.array(emotions)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    return train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42), le, scaler\n",
        "\n",
        "# Load dataset and train models\n",
        "(X_train, X_test, y_train, y_test), label_encoder, scaler = load_data()\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    trained_models[name] = model\n",
        "\n",
        "# ----------- Prediction Function ------------\n",
        "import pandas as pd\n",
        "\n",
        "def predict_emotion(audio_file, model_name):\n",
        "    if model_name not in trained_models:\n",
        "        return \"Model not found.\", None\n",
        "\n",
        "    model = trained_models[model_name]\n",
        "    feature = extract_features(audio_file)\n",
        "    feature_scaled = scaler.transform([feature])\n",
        "    proba = model.predict_proba(feature_scaled)[0]\n",
        "    emotion_labels = label_encoder.inverse_transform(np.arange(len(proba)))\n",
        "\n",
        "    # Create DataFrame for sorting and table output\n",
        "    df = pd.DataFrame({\n",
        "        \"Emotion\": emotion_labels,\n",
        "        \"Confidence (%)\": (proba * 100).round(2)\n",
        "    }).sort_values(by=\"Confidence (%)\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Bar chart with unique colors\n",
        "    fig, ax = plt.subplots()\n",
        "    bar_colors = plt.cm.Set3(np.linspace(0, 1, len(proba)))\n",
        "    ax.bar(df[\"Emotion\"], df[\"Confidence (%)\"], color=bar_colors)\n",
        "    ax.set_ylabel('Probability (%)')\n",
        "    ax.set_title('Emotion Prediction Confidence')\n",
        "    ax.set_ylim([0, 100])\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    return df, fig\n",
        "\n",
        "# ----------- Gradio Interface ------------\n",
        "interface = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=[\n",
        "        gr.Audio(type=\"filepath\", label=\"Upload Audio (.wav)\"),\n",
        "        gr.Dropdown(choices=list(trained_models.keys()), label=\"Select Model\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Text(label=\"Prediction Table\"),\n",
        "        gr.Plot(label=\"Emotion Probabilities (Bar Chart)\")\n",
        "    ],\n",
        "    title=\"üéß Voice Emotion Recognition Dashboard\",\n",
        "    description=\"Upload a .wav file and select a model to get emotion prediction with confidence chart.\"\n",
        ")\n",
        "\n",
        "\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "M_wXZJxBUmpW",
        "outputId": "e634e9c6-7a05-4010-a2c6-9f49060f07ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://85b27ffcc415367e1c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://85b27ffcc415367e1c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}